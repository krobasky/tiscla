
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Welcome to sbr’s documentation! &#8212; sbr v0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="module-sbr.compile">
<span id="welcome-to-sbr-s-documentation"></span><h1>Welcome to sbr’s documentation!<a class="headerlink" href="#module-sbr.compile" title="Permalink to this heading">¶</a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="sbr.compile.one_layer_multicategorical">
<span class="sig-prename descclassname"><span class="pre">sbr.compile.</span></span><span class="sig-name descname"><span class="pre">one_layer_multicategorical</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">specificityAtSensitivityThreshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sensitivityAtSpecificityThreshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">tensorflow.keras.initializers.HeNormal</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">tensorflow.zeros_initializer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'softmax'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">isMultilabel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sbr.compile.one_layer_multicategorical" title="Permalink to this definition">¶</a></dt>
<dd><p>Compile a single layer multicategorical model. Can use <cite>sbr.visualize.plot_loss_curve</cite> to see the metrics after fitting</p>
<dl class="simple">
<dt>Args:</dt><dd><p>input_size [None]: usually <cite>x_train.shape[1]</cite>; not required for compile, but for calling <cite>model.summary()</cite>
output_size [None]: number of classes in the one-hot-encoded target vector; usually <cite>y_train.shape[1]</cite>
learning_rate [0.0001]: Plan for this to be reduced during <cite>EarlyStopping</cite> checkpoints in the model training/fit
dim [1000]: Number of nodes to have in the hidden layer. Somthing half-way between input_size and output_size is a good choice, but if input_size is very big, the number may need to be smaller in order to reduce the number of trainable parameters and avoid over-fitting.
specificityAtSensitivityThreshold [0.50]: With this percentage of sensitivity (e.g., detecting at least this many true positives), find the specificity (e.g., how many identified will actually be correct). This is a bit trickier for multivariate problems, see: <a class="reference external" href="https://www.analyticsvidhya.com/blog/2021/06/confusion-matrix-for-multi-class-classification/">https://www.analyticsvidhya.com/blog/2021/06/confusion-matrix-for-multi-class-classification/</a>
sensitivityAtSpecificityThreshold [0.50]:
kernel_initializer [tf.keras.initializers.HeNormal()]: HeNormal initializer forces diversity of outcomes between trainings
bias_initializder  [ tf.zeros_initializer()]:
output_activation [‘softmax’]:
isMultilabel [True]: Should alwasy be True for multicategorical models
verbose [True)]: If True, print model summary. Set to False if input_size = None to avoid error</p>
</dd>
</dl>
<p>Returns:</p>
<dl class="simple">
<dt>Example usage:</dt><dd><dl class="simple">
<dt>model = compile.one_layer_multicategorical(input_size=x_train.shape[1],</dt><dd><p>output_size=y_train.shape[1],
output_activation=’softmax’,
learning_rate=0.0001,
isMultilabel=True,
dim=1000,
specificityAtSensitivityThreshold=0.50,
sensitivityAtSpecificityThreshold=0.50,
verbose=True)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-sbr.evaluate"></span><dl class="py function">
<dt class="sig sig-object py" id="sbr.evaluate.compare_predictions">
<span class="sig-prename descclassname"><span class="pre">sbr.evaluate.</span></span><span class="sig-name descname"><span class="pre">compare_predictions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_test</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_test</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sbr.evaluate.compare_predictions" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts y_test from x_test using model, then compares predictions with truth.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>model: the model to use <cite>model.predict</cite>
x_test: test features
y_test: targets
class_names: an ordered list of class name strings that map to the <cite>np.argmax(y_test,axis=1)</cite> indices in y_test. If none, class indices will be reported instead of strng names.
verbose: if verbose, pairs are printed out (good if there aren’t a lot of mislabeled predictions)</p>
</dd>
<dt>Returns:</dt><dd><p>(y_pred, pairs) where
y_pred: the predicted outcomes from x_test
pairs: list pairs of (&lt;truth&gt;&lt;false-prediction&gt;) class names</p>
</dd>
<dt>Exampe usasge:</dt><dd><p>y_pred, pairs = compare_predictions(model=model, x_test=x_test, y_test=ytest, class_names=class_names, verbose = True)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sbr.evaluate.mislabeled_pair_counts">
<span class="sig-prename descclassname"><span class="pre">sbr.evaluate.</span></span><span class="sig-name descname"><span class="pre">mislabeled_pair_counts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_names</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sbr.evaluate.mislabeled_pair_counts" title="Permalink to this definition">¶</a></dt>
<dd><p>For multicategorical models: creates a table of observed,
predicted class names for the mispredicted observations. This
tends to use a lot of memory on multiple runs in a jupyter
notebook, with tensorflow 2.6. May need to restart the kernel on
second run. If resources continue to be a problem after restarting
the kernel, reduce the batch_size.</p>
<p>Assumes y_pred, y_obs are one-hot encoded and class_names matches the index predictions returned from np.argmax(y_pred)</p>
<dl class="simple">
<dt>Args:</dt><dd><p>model: used for model.predict
X: feature values
y: one-hot encoded true labels
class_names: ordered list of class_names
sample_ids: pass in this Series object to get back a table of pairs with their sample_ids
batch_size: number of samples to process in each step (to keep from swamping memory)
verbose: helps with debugging; messages each step/batch</p>
</dd>
<dt>Returns:</dt><dd><p>(pairs_counts, pair_id_map) where
pairs_counts: Table with compound index ‘observed’,’predicted’ and one column, “counts”, with the count of all the
the samples in that observed/predicted mislabeled pair.
pair_id_map: None if sample_ids wasn’t passed in, otherwise returns a table with columns observed, predicted, sample_id</p>
</dd>
<dt>Example Usage:</dt><dd><dl class="simple">
<dt>mislabeled_counts, mislabeled = mislabeled_pair_counts(model=model, X=X, y=y, class_names=class_names,</dt><dd><p>sample_ids = pd.Series(label_df[“sample_id”]),
batch_size=500)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sbr.evaluate.training_report">
<span class="sig-prename descclassname"><span class="pre">sbr.evaluate.</span></span><span class="sig-name descname"><span class="pre">training_report</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_test</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_test</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sensitivityAtSpecificityThreshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">specificityAtSensitivityThreshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sbr.evaluate.training_report" title="Permalink to this definition">¶</a></dt>
<dd><p>Calls <cite>model.evaluate(x_test,y_test)</cite> and, if verbose, reports on the performance, then returns a performance object.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>x_test: features
y_test: targets
verbose: if True, report to stdout
sensitivityAtSpecificityThreshold: If not None, and verbose, and this metric was captured in model.fit, report it to stdout
specificityAtSensitivityThreshold: see above</p>
</dd>
<dt>Returns:</dt><dd><p>performance object from the model.evaluate function; see <cite>Returns</cite> for <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate">https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate</a></p>
</dd>
<dt>Example useage:</dt><dd><dl class="simple">
<dt>performance = training_report(model, x_test, y_test,</dt><dd><p>sensitivityAtSpecificityThreshold=sensitivityAtSpecificityThreshold,
specificityAtSensitivityThreshold=specificityAtSensitivityThreshold,
verbose=True)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-sbr.fit"></span><dl class="py function">
<dt class="sig sig-object py" id="sbr.fit.multicategorical_model">
<span class="sig-prename descclassname"><span class="pre">sbr.fit.</span></span><span class="sig-name descname"><span class="pre">multicategorical_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_folder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_validation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_validation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_patience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sbr.fit.multicategorical_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the given model with the given hyperparameters and multi-categorical data, after computing class weights and shuffling the data. Writes checkpoint and final model weights to model_folder. Look under variables/variables.* for weights. Reload with: <cite>model=load_model(‘f{model_path}’); model.load_weights(f”{model_folder}”)</cite></p>
<dl class="simple">
<dt>Assumptions:</dt><dd><ul class="simple">
<li><p>Model has been compiled and saved to f”{model_path}.h5” (e.g., <cite>data/model/gtex/manual/gtex_model.h5</cite>)</p></li>
<li><p>Targets are one-hot encoded</p></li>
<li><p>Features have been normalized</p></li>
</ul>
</dd>
</dl>
<p>Tested with tensorflow v2.6.2, keras 2.6.0</p>
<dl class="simple">
<dt>Args:</dt><dd><p>model: a compiled model
model_folder: writable folder to store the checkpoint and final model weights
x_train: training features, see sbr.split for help
y_train: training targets,  see above
x_validation: validation feature, see above
y_validation: validation feature, see above
epochs [200]: Number of epochs to train
patience [4]: Number of epochs with no improvement after which training will be stopped.
lr_patience [2]: Number of epochs with no improvement after which learning rate will be reduced.
lr_factor [0.1]: Factor by which the learning rate will be reduced. new_lr = lr * factor.
batch_size [32]: probably don’t change this, see: <a class="reference external" href="https://wandb.ai/ayush-thakur/dl-question-bank/reports/What-s-the-Optimal-Batch-Size-to-Train-a-Neural-Network---VmlldzoyMDkyNDU">https://wandb.ai/ayush-thakur/dl-question-bank/reports/What-s-the-Optimal-Batch-Size-to-Train-a-Neural-Network—VmlldzoyMDkyNDU</a>
shuffle_value [100]:
initial_epoch [0]: use this if you want to resume training at a particular epoch
train_verbose [0]: amount of information to print on each epoch. for 0: silent, 1: animited progress bar, 2: mentions epoch. For example:
* 0: &lt;silent&gt;
* 1: [==================]
* 2: Epoch 1/10
checkpoint_verbose [1]: amount of information to print on each epoch about the checkpoint. 0: silent.
* 0: &lt;silent&gt;
* 1: Epoch 00015: val_loss improved from 0.06645 to 0.06611, saving model to data/model/gtex
<a class="reference external" href="INFO:tensorflow:Assets">INFO:tensorflow:Assets</a> written to: data/model/gtex/assets</p>
</dd>
<dt>Returns:</dt><dd><p>history: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Use <cite>print(history.history.keys())</cite> to see all the hist and <cite>print(history.history[‘val_loss’])</cite> to print validation loss</p>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-sbr.layers"></span><dl class="py class">
<dt class="sig sig-object py" id="sbr.layers.BADBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sbr.layers.</span></span><span class="sig-name descname"><span class="pre">BADBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sbr.layers.BADBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Dense layer followed by Batch, Activation, Dropout. When popular
kwarg input_shape is passed, then will create a keras input layer
to insert before the current layer to avoid explicitly defining an
InputLayer.</p>
<p># Recreate this layer from its config:
layer = BADBlock(1000)
config = layer.get_config()
print(config)
new_layer = BADBlock.from_config(config)</p>
<p># use in a model:
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from sbr.layers import BADBlock
model = Sequential()
model.add(BADBlock(1000, input_dim = 18963, activation=’relu’, dropout_rate=0.50, name=”BAD_1”))
model.add(Dense(26, activation=”softmax”))
model.summary()
model.compile(loss=’categorical_crossentropy’,optimizer=’adam’,metrics=[‘accuracy’,’mse’])</p>
</dd></dl>

<span class="target" id="module-sbr.model"></span><p>! [sbr.model.save] ERROR: model not saved. Exception (&lt;class ‘ValueError’&gt;) : Unknown layer: BADBlock. Please ensure this object is passed to the <cite>custom_objects</cite> argument. See <a class="reference external" href="https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object">https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object</a> for details.
! [sbr.model.save]    model_path=data/model/gtex/manual, file_name=gtex_model.h5, full_path=data/model/gtex/manual/gtex_model.h5</p>
<dl class="py function">
<dt class="sig sig-object py" id="sbr.model.save_architecture">
<span class="sig-prename descclassname"><span class="pre">sbr.model.</span></span><span class="sig-name descname"><span class="pre">save_architecture</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'model.h5'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sbr.model.save_architecture" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the given model to the given path and name. It’s a good idea
to train and then run this in a notebook if possible so the train
model is resident in memory because this function can be tried
again in case it fails for some reason.</p>
<p>WARNING: THIS WILL OVER-WRITE ANY EXISTING MODEL.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>model: model object for calling <cite>model.save</cite>
model_path[None]: file path where model is to be written
file_name[“model.h5”]: name of the file, h5 format. Any exisiting file will be over-written.
input_size: if not None, attempts to check predictions on saved model are close to original model
verbose[1]: 0: debug, 1:print out model summary. This may throw an error if model wasn’t compiled with a known input size</p>
</dd>
<dt>Returns:</dt><dd><p>True on success, False otherwise. Check the return to try again if it fails while model is still resident in memory.</p>
</dd>
<dt>Example usage:</dt><dd><p>success = sbf.model.save(model, model_path=”data/model/manual”, file_name=”model.h5”, verbose=1)</p>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-sbr.visualize"></span><dl class="py function">
<dt class="sig sig-object py" id="sbr.visualize.plot_cm">
<span class="sig-prename descclassname"><span class="pre">sbr.visualize.</span></span><span class="sig-name descname"><span class="pre">plot_cm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_test</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">figsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(10,</span> <span class="pre">10)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labelsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">textsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sbr.visualize.plot_cm" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes a labelled confusion matrix comparing predictions and ground truth labels.</p>
<p>If classes is passed, confusion matrix will be labelled, if not, integer class values
will be used.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>y_test: Array of truth labels (must be same shape as y_pred).
y_pred: Array of predicted labels (must be same shape as y_true).
figsize: Size of output figure (default=(10, 10)).
label_size: Size of label text (default=20).
text_size: Size of output figure text (default=15).
classes: Array of class labels (e.g. string form). If <cite>None</cite>, integer labels are used.</p>
</dd>
<dt>Returns:</dt><dd><p>A labelled confusion matrix plot comparing y_test and y_pred.</p>
</dd>
<dt>Example usage:</dt><dd><dl class="simple">
<dt>make_confusion_matrix(y_test=test_labels, # ground truth test labels</dt><dd><p>y_pred=y_preds, # predicted labels
classes=class_names, # array of class label names
figsize=(15, 15),
text_size=10)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sbr.visualize.plot_loss_curve">
<span class="sig-prename descclassname"><span class="pre">sbr.visualize.</span></span><span class="sig-name descname"><span class="pre">plot_loss_curve</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">history</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">figsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(5,</span> <span class="pre">5)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['loss',</span> <span class="pre">'accuracy',</span> <span class="pre">'val_accuracy']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">write_directory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'data/images'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'nn-loss-curve.png'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_plot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sbr.visualize.plot_loss_curve" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot a loss, accuracy curve. Assumes loss and accuracy were compiled into the model metrics.
If this is running in a notebook, the <cite>plt.show()</cite> command doesn’t matter and the plot will just who no matter what</p>
<dl class="simple">
<dt>Args:</dt><dd><p>history: history object returned from model.fit (or sbr.fit.multicategorical_model)
figsize [(5,5)]: tuple for the size of the figure
metrics []: traces to plot
write_directory [[‘loss’,’accuracy’]]: where to write out the figure (if None, nothing is saved)
file_name [“data/images”]: override filename of figure to be written
show [True]: if True, show to figure to display</p>
</dd>
<dt>Returns:</dt><dd><p>plots to display if show= True, saves image if write_directory not None</p>
</dd>
<dt>Example usage:</dt><dd><dl class="simple">
<dt>plot_loss_curve(figsize=(5,5)</dt><dd><p>history, [‘loss’,’accuracy’,’val_accuracy’],
write_directory=”data/images”,
show=True)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-sbr.preprocessing.dataset"></span><dl class="py function">
<dt class="sig sig-object py" id="sbr.preprocessing.dataset.multicategorical_split">
<span class="sig-prename descclassname"><span class="pre">sbr.preprocessing.dataset.</span></span><span class="sig-name descname"><span class="pre">multicategorical_split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_count_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sbr.preprocessing.dataset.multicategorical_split" title="Permalink to this definition">¶</a></dt>
<dd><p>Shuffles and splits X, y into test, train, validate; round dataset sizes to be a factor of batch_size.</p>
<p>Final dataset size is (sample_count_threshold * &lt;number of classes&gt;)</p>
<p>see also: sbr.preprocessing.gtex.dataset_setup</p>
<dl class="simple">
<dt>Args:</dt><dd><p>X: Features
y: multicategorical targets (more than one column)
sample_count_threshold: use about this many samples from each class
seed[None]: set this to make function deterministic/repeatable
shuffle[True]: probably don’t touch this. Shuffling the data really helps down-stream model training.</p>
</dd>
<dt>Returns:</dt><dd><p>(x_train, y_train, x_val, y_val, x_test, y_test)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sbr.preprocessing.dataset.trim_list_size_to_batch_size_factor">
<span class="sig-prename descclassname"><span class="pre">sbr.preprocessing.dataset.</span></span><span class="sig-name descname"><span class="pre">trim_list_size_to_batch_size_factor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trim_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sbr.preprocessing.dataset.trim_list_size_to_batch_size_factor" title="Permalink to this definition">¶</a></dt>
<dd><p>Trims the given list of multicategorical arrays down to a factor
of the given batch_size. This can avoid errors during training
when the dataset is very large, a small amount of data loss isn’t
a factor, and retaining a specfic batch_size (e.g., of 32) is
prefered .</p>
<dl class="simple">
<dt>Args:</dt><dd><p>trim_list: a list of arrays to be trimmed
batch_size[32]: probably leave this alone</p>
</dd>
<dt>Returns:</dt><dd><p>the same trim_list, but trimmed</p>
</dd>
<dt>Usage:</dt><dd><p>[x_train, y_train, x_val, y_val, x_test, y_test] = trim_list_size_to_batch_size_factor([x_train, y_train, x_val, y_val, x_test, y_test])</p>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-sbr.preprocessing.gtex"></span><dl class="py function">
<dt class="sig sig-object py" id="sbr.preprocessing.gtex.dataset_setup">
<span class="sig-prename descclassname"><span class="pre">sbr.preprocessing.gtex.</span></span><span class="sig-name descname"><span class="pre">dataset_setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_count_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expr_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'data/gtex/expr.ftr'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attr_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'dist/gtex/GTEx_Analysis_v8_Annotations_SampleAttributesDS.txt'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_classes_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attr_class_name_column_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SMTS'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attr_sample_id_column_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SAMPID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expr_sample_id_column_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sample_id'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sbr.preprocessing.gtex.dataset_setup" title="Permalink to this definition">¶</a></dt>
<dd><p>Reads the expression and attribute feather files, normalizes the expression values, one-hot encodes the classes, and returns the features, targets and labels in coordinated order.</p>
<dl class="simple">
<dt>The return from this function (X, y) can be split as such:</dt><dd><p><cite>x_train, x_test,y_train, y_test = sklearn.model_selection.train_test_split(X, np.array(y), test_size=1.-fraction, random_state=42, shuffle=True)</cite></p>
</dd>
<dt>Class names can be retrieved from the returned target aray (y) and ordered list of class names (class_names) as such:</dt><dd><p><cite>class_names[np.argmax[y]]</cite></p>
</dd>
<dt>Args:</dt><dd><p>sample_count_threshold: drop any classes that are less than this threshold. If ‘None’, don’t drop any classes
expr_path:
attr_path:
drop_classes_list:
attr_class_name_column_name:
attr_sample_id_column_name:
expr_sample_id_column_name:
verbose:</p>
</dd>
<dt>Returns:</dt><dd><p>X: Normalized feature values
y: One-hot encoded target values
class_names: Ordered list of strings, one item per class. This will be handy for understanding the predictions
label_df:</p>
</dd>
<dt>Example usage;</dt><dd><p>X, y, class_names, label_df = dataset_setup(100)</p>
</dd>
</dl>
</dd></dl>

<div class="toctree-wrapper compound">
</div>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this heading">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="#">sbr</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, K. Robasky.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.0.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>